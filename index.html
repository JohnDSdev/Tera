<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Hologram AI Voice Chat</title>
<style>
body {
  margin: 0;
  height: 100vh;
  background: #050510;
  color: #7df;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  font-family: monospace;
}
#avatar {
  width: 64px;
  height: 128px;
  image-rendering: pixelated;
  filter: drop-shadow(0 0 8px #0ff);
  opacity: 0;
  transition: opacity 0.3s ease;
}
#status {
  margin-bottom: 8px;
}
button {
  margin: 5px;
  padding: 8px 16px;
  border: none;
  border-radius: 5px;
  font-weight: bold;
  cursor: pointer;
  background: #0ff;
  color: #000;
}
button:disabled { opacity: 0.4; }
</style>
</head>
<body>
<div id="status">Status: idle</div>
<img id="avatar" alt="avatar" src="arm_behind_mouth_closed.png">
<div>
  <button id="connect">Connect</button>
  <button id="talk" disabled>Hold to Talk</button>
  <button id="end" disabled>End</button>
</div>

<script type="module">
import { GoogleGenAI, Modality } from "https://aistudiocdn.com/@google/genai@1.24.0";

const avatar = document.getElementById("avatar");
const statusEl = document.getElementById("status");
const connectBtn = document.getElementById("connect");
const talkBtn = document.getElementById("talk");
const endBtn = document.getElementById("end");

const frames = {
  armClosed: "arm_out_mouth_closed.png",
  armOpen: "arm_out_mouth_open.png",
  backClosed: "arm_behind_mouth_closed.png",
  backOpen: "arm_behind_mouth_open.png"
};
const startupFrames = ["a1.png","a2.png","a3.png","a4.png","a5.png","a6.png","a7.png"];

let ai, session, micStream;
let inCtx, outCtx, analyser;
let recording = false;
let pose = 0;
let mouthTimer;
let startupDone = false;

function setStatus(t){ statusEl.textContent = "Status: " + t; }

function setFrame(open){
  if (pose === 0)
    avatar.src = open ? frames.backOpen : frames.backClosed;
  else
    avatar.src = open ? frames.armOpen : frames.armClosed;
}

function startupAnimation(){
  let i = 0;
  avatar.style.opacity = 1;
  const next = () => {
    if (i < startupFrames.length) {
      avatar.src = startupFrames[i++];
      setTimeout(next, 90);
    } else {
      startupDone = true;
      setFrame(false);
    }
  };
  next();
}

connectBtn.onclick = async () => {
  const key = prompt("Enter Gemini API key:");
  if(!key) return setStatus("No API key");
  setStatus("Connecting...");
  ai = new GoogleGenAI({ apiKey: key });
  inCtx = new AudioContext({sampleRate:16000});
  outCtx = new AudioContext({sampleRate:24000});
  analyser = outCtx.createAnalyser();
  analyser.connect(outCtx.destination);
  micStream = await navigator.mediaDevices.getUserMedia({audio:true});
  const src = inCtx.createMediaStreamSource(micStream);
  const proc = inCtx.createScriptProcessor(4096,1,1);
  src.connect(proc); proc.connect(inCtx.destination);

  session = await ai.live.connect({
    model:"gemini-2.5-flash-native-audio-preview-09-2025",
    callbacks:{
      onopen:()=>setStatus("Connected"),
      onmessage:handleMsg,
      onerror:e=>setStatus("Error: "+e.message),
      onclose:()=>setStatus("Closed")
    },
    config:{
      responseModalities:[Modality.AUDIO],
      speechConfig:{voiceConfig:{prebuiltVoiceConfig:{voiceName:"Zephyr"}}}
    }
  });

  proc.onaudioprocess = e=>{
    if(!recording)return;
    const d=e.inputBuffer.getChannelData(0);
    const i16=new Int16Array(d.length);
    for(let i=0;i<d.length;i++)i16[i]=d[i]*32768;
    const b64=btoa(String.fromCharCode(...new Uint8Array(i16.buffer)));
    session.sendRealtimeInput({media:{data:b64,mimeType:"audio/pcm;rate=16000"}});
  };

  connectBtn.disabled = true;
  talkBtn.disabled = false;
  endBtn.disabled = false;
  avatar.src = frames.backClosed;
  avatar.style.opacity = 0.3;
};

function handleMsg(msg){
  const data = msg.serverContent?.modelTurn?.parts?.[0]?.inlineData?.data;
  if(!data)return;
  const bytes = Uint8Array.from(atob(data),c=>c.charCodeAt(0));
  const int16 = new Int16Array(bytes.buffer);
  const buf = outCtx.createBuffer(1,int16.length,24000);
  const ch = buf.getChannelData(0);
  for(let i=0;i<int16.length;i++) ch[i]=int16[i]/32768;
  const src = outCtx.createBufferSource();
  src.buffer=buf; src.connect(analyser);
  src.start();
  startMouth();
  src.onended = stopMouth;
}

function startMouth(){
  if(!startupDone){startupAnimation();}
  if(mouthTimer)return;
  mouthTimer=setInterval(()=>{
    const arr=new Uint8Array(analyser.frequencyBinCount);
    analyser.getByteTimeDomainData(arr);
    let sum=0;
    for(let i=0;i<arr.length;i++){const f=(arr[i]-128)/128; sum+=f*f;}
    const rms=Math.sqrt(sum/arr.length);
    const open=rms>0.02;
    if(rms>0.05) pose=pose?0:1;
    setFrame(open);
  },100);
}
function stopMouth(){
  if(mouthTimer){clearInterval(mouthTimer);mouthTimer=null;}
  setFrame(false);
}

talkBtn.onmousedown=()=>{
  recording=true;
  setStatus("Listening...");
  if(!startupDone)startupAnimation();
};
talkBtn.onmouseup=()=>{
  recording=false;
  setStatus("Processing...");
};
talkBtn.ontouchstart=talkBtn.onmousedown;
talkBtn.ontouchend=talkBtn.onmouseup;

endBtn.onclick=()=>{
  setStatus("Ended");
  if(session) session.close();
  if(micStream) micStream.getTracks().forEach(t=>t.stop());
  talkBtn.disabled=true; endBtn.disabled=true;
};
</script>
</body>
</html>
