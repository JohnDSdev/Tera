    </main>

    <footer>
      <div class="talk-controls">
        <button id="talkBtn" disabled>Hold to talk</button>
        <span class="hint" id="hintText">Connect with your API key to begin.</span>
      </div>
    </footer>
  </div>

  <audio id="responseAudio"></audio>

  <script>
    const apiKeyInput = document.getElementById('apiKey');
    const connectBtn = document.getElementById('connectBtn');
    const talkBtn = document.getElementById('talkBtn');
    const statusIndicator = document.getElementById('statusIndicator');
    const statusText = document.getElementById('statusText');
    const messagesEl = document.getElementById('messages');
    const hintText = document.getElementById('hintText');
    const avatarEl = document.getElementById('avatar');
    const responseAudio = document.getElementById('responseAudio');

    const startupFrames = ['a1.png', 'a2.png', 'a3.png', 'a4.png', 'a5.png', 'a6.png', 'a7.png'];
    const idleFrame = 'arm_behind_mouth_closed.png';
    const calmFrames = ['arm_behind_mouth_open.png', 'arm_behind_mouth_closed.png'];
    const emphasisedFrames = ['arm_out_mouth_open.png', 'arm_out_mouth_closed.png'];
    const animationFrames = [...new Set([...calmFrames, ...emphasisedFrames])];

    const GEMINI_CHAT_MODEL = 'gemini-2.5-flash';
    const GEMINI_TTS_MODEL = 'gemini-2.5-flash-preview-tts';
    const GEMINI_BASE_URL = 'https://generativelanguage.googleapis.com/v1beta/models';

    const imageCache = new Map();
    [...startupFrames, idleFrame, ...animationFrames].forEach(src => {
      const img = new Image();
      img.src = src;
      imageCache.set(src, img);
    });

    let apiKey = '';
    let connected = false;
    let mediaStream = null;
    let mediaRecorder = null;
    let chunks = [];
    let isRecording = false;
    let audioContext = null;
    let responseAudioSource = null;
    let responseAudioAnalyser = null;
    let avatarAnimation = null;
    let currentAudioObjectUrl = null;
    let recordedMimeType = 'audio/webm';

    const systemInstruction = {
      role: 'system',
      parts: [{ text: 'You are a futuristic but friendly AI guide. Keep answers under 120 words and respond conversationally to the user.' }]
    };

    const conversationHistory = [];

    function updateStatus(online, text) {
      statusIndicator.classList.toggle('online', Boolean(online));
      statusText.textContent = text;
    }

    function addMessage(role, text, note) {
      const bubble = document.createElement('div');
      bubble.className = `bubble ${role}`;
      const textEl = document.createElement('span');
      textEl.className = 'bubble-text';
      textEl.textContent = text;
      bubble.appendChild(textEl);
      let noteEl = null;
      if (note) {
        noteEl = document.createElement('div');
        noteEl.className = 'transcript-note';
        noteEl.textContent = note;
        bubble.appendChild(noteEl);
      }
      messagesEl.appendChild(bubble);
      messagesEl.scrollTop = messagesEl.scrollHeight;
      return { bubble, textEl, noteEl };
    }

    function setAvatar(frame, speaking = false) {
      avatarEl.src = frame;
      avatarEl.classList.toggle('speaking', speaking);
    }

    function startAvatarSpeechAnimation({ analyser } = {}) {
      stopAvatarSpeechAnimation();

      const state = {
        analyser: analyser || null,
        dataArray: analyser ? new Uint8Array(analyser.fftSize) : null,
        emphasisLevel: 0,
        manualEmphasisUntil: 0,
        frameSet: calmFrames,
        frameIndex: 0,
        lastFrameTime: 0,
        rafId: null
      };

      function setManualEmphasis(active, holdMs = 260) {
        state.manualEmphasisUntil = active ? performance.now() + holdMs : 0;
      }

      const initialFrame = state.frameSet[state.frameIndex];
      setAvatar(initialFrame, initialFrame.includes('open'));

      function loop(now) {
        let emphasised = false;

        if (state.analyser && state.dataArray) {
          state.analyser.getByteTimeDomainData(state.dataArray);
          let sumSquares = 0;
          for (let i = 0; i < state.dataArray.length; i++) {
            const sample = (state.dataArray[i] - 128) / 128;
            sumSquares += sample * sample;
          }
          const rms = Math.sqrt(sumSquares / state.dataArray.length);
          state.emphasisLevel = state.emphasisLevel * 0.6 + rms * 0.4;
          emphasised = state.emphasisLevel > 0.22;
        }

        if (state.manualEmphasisUntil && now < state.manualEmphasisUntil) {
          emphasised = true;
        }

        const targetFrames = emphasised ? emphasisedFrames : calmFrames;
        if (state.frameSet !== targetFrames) {
          state.frameSet = targetFrames;
          state.frameIndex = 0;
          state.lastFrameTime = 0;
        }

        const frameDelay = emphasised ? 130 : 200;
        if (!state.lastFrameTime || now - state.lastFrameTime >= frameDelay) {
          const frame = state.frameSet[state.frameIndex];
          setAvatar(frame, frame.includes('open'));
          state.frameIndex = (state.frameIndex + 1) % state.frameSet.length;
          state.lastFrameTime = now;
        }

        state.rafId = requestAnimationFrame(loop);
      }

      state.rafId = requestAnimationFrame(loop);
      avatarAnimation = {
        state,
        setManualEmphasis,
      };
      return avatarAnimation;
    }

    function stopAvatarSpeechAnimation() {
      if (avatarAnimation?.state?.rafId) {
        cancelAnimationFrame(avatarAnimation.state.rafId);
      }
      avatarAnimation = null;
      setAvatar(idleFrame, false);
    }

    function ensureResponseAudioAnalyser() {
      if (!audioContext) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
      }
      if (!audioContext) {
        return null;
      }
      if (!responseAudioSource) {
        responseAudioSource = audioContext.createMediaElementSource(responseAudio);
        responseAudioAnalyser = audioContext.createAnalyser();
        responseAudioAnalyser.fftSize = 1024;
        responseAudioAnalyser.smoothingTimeConstant = 0.65;
        responseAudioSource.connect(responseAudioAnalyser);
        responseAudioAnalyser.connect(audioContext.destination);
      }
      return responseAudioAnalyser;
    }

    function computeEmphasisRanges(text) {
      const ranges = [];
      if (!text) return ranges;
      const wordRegex = /\b[\w']+\b/g;
      let match;
      while ((match = wordRegex.exec(text)) !== null) {
        const start = match.index;
        const end = start + match[0].length;
        const word = match[0];
        const trailingChar = text.slice(end, end + 1);
        const isUpper = word.length > 2 && word === word.toUpperCase();
        const hasPunctuation = /[!?]/.test(trailingChar);
        if (isUpper || hasPunctuation) {
          ranges.push({ start, end });
        }
      }
      return ranges;
    }

    async function playStartupSequence() {
      await playStartupSound();
      for (let i = 0; i < startupFrames.length; i++) {
        setAvatar(startupFrames[i]);
        await wait(160);
      }
      setAvatar(idleFrame);
    }

    async function playStartupSound() {
      try {
        if (!audioContext) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        const osc = audioContext.createOscillator();
        const osc2 = audioContext.createOscillator();
        const gain = audioContext.createGain();
        osc.type = 'sawtooth';
        osc.frequency.setValueAtTime(320, audioContext.currentTime);
        osc.frequency.exponentialRampToValueAtTime(880, audioContext.currentTime + 1.1);
        osc2.type = 'triangle';
        osc2.frequency.setValueAtTime(120, audioContext.currentTime);
        osc2.frequency.linearRampToValueAtTime(420, audioContext.currentTime + 1.1);
        gain.gain.setValueAtTime(0.0001, audioContext.currentTime);
        gain.gain.exponentialRampToValueAtTime(0.6, audioContext.currentTime + 0.2);
@@ -510,50 +599,61 @@
        connectBtn.disabled = false;
        hintText.textContent = 'Microphone permission denied.';
        return;
      }

      await playStartupSequence();
      setupRecorder();
      connected = true;
      connectBtn.textContent = 'Disconnect';
      connectBtn.disabled = false;
      talkBtn.disabled = false;
      hintText.textContent = 'Hold the button, speak, then release to send.';
      updateStatus(true, 'Ready');
      conversationHistory.length = 0;
    });

    function disconnect() {
      connected = false;
      apiKey = '';
      connectBtn.textContent = 'Connect';
      talkBtn.disabled = true;
      connectBtn.disabled = false;
      hintText.textContent = 'Connect with your API key to begin.';
      updateStatus(false, 'Disconnected');
      stopAvatarSpeechAnimation();
      if (!responseAudio.paused) {
        responseAudio.pause();
        responseAudio.currentTime = 0;
      }
      if (currentAudioObjectUrl) {
        URL.revokeObjectURL(currentAudioObjectUrl);
        currentAudioObjectUrl = null;
      }
      if ('speechSynthesis' in window) {
        speechSynthesis.cancel();
      }
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      conversationHistory.length = 0;
    }

    function setupRecorder() {
      if (!mediaStream) return;
      const preferredTypes = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/mp4'
      ];
      let chosenType = null;
      if (typeof MediaRecorder !== 'undefined' && typeof MediaRecorder.isTypeSupported === 'function') {
        chosenType = preferredTypes.find(type => MediaRecorder.isTypeSupported(type)) || null;
      }

      try {
@@ -684,120 +784,178 @@
        parts: item.parts.map(part => ({ ...part }))
      }));
    }

    async function playGeminiAudio(text) {
      try {
        const audioPart = await synthesizeWithGemini(text);
        if (audioPart) {
          await playResponseAudio(audioPart);
          return;
        }
      } catch (err) {
        console.warn('Gemini TTS unavailable, falling back to speech synthesis.', err);
      }
      await speakWithSpeechSynthesis(text);
    }

    async function synthesizeWithGemini(text) {
      const body = {
        contents: [
          {
            role: 'user',
            parts: [{ text }]
          }
        ],
        generationConfig: {
          responseMimeType: 'audio/ogg; codecs=opus',
          responseAudioConfig: {
            voiceConfig: {
              prebuiltVoiceConfig: { voiceName: 'Kore' }
            }
          }
        }
      };

      const response = await fetch(`${GEMINI_BASE_URL}/${encodeURIComponent(GEMINI_TTS_MODEL)}:generateContent?key=${encodeURIComponent(apiKey)}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(body)
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error('TTS failed: ' + errorText);
      }

      const data = await response.json();
      const parts = data?.candidates?.[0]?.content?.parts || [];
      const audioParts = parts.filter(part => part.inlineData && part.inlineData.data);
      if (!audioParts.length) {
        throw new Error('TTS response did not contain audio data.');
      }
      const combinedData = audioParts.map(part => part.inlineData.data).join('');
      return {
        data: combinedData,
        mimeType: audioParts[0].inlineData.mimeType || 'audio/ogg; codecs=opus'
      };
    }

    async function playResponseAudio(audioPart) {
      const buffer = base64ToArrayBuffer(audioPart.data);
      const blob = new Blob([buffer], { type: audioPart.mimeType || 'audio/ogg; codecs=opus' });
      const url = URL.createObjectURL(blob);

      if (!responseAudio.paused) {
        responseAudio.pause();
        responseAudio.currentTime = 0;
      }
      if (currentAudioObjectUrl) {
        URL.revokeObjectURL(currentAudioObjectUrl);
        currentAudioObjectUrl = null;
      }
      if ('speechSynthesis' in window) {
        speechSynthesis.cancel();
      }

      responseAudio.src = url;
      currentAudioObjectUrl = url;

      const analyser = ensureResponseAudioAnalyser();
      if (audioContext && audioContext.state === 'suspended') {
        try {
          await audioContext.resume();
        } catch (err) {
          console.warn('Unable to resume audio context', err);
        }
      }

      startAvatarSpeechAnimation({ analyser });

      await new Promise((resolve, reject) => {
        let finished = false;

        const cleanup = () => {
          if (finished) return;
          finished = true;
          responseAudio.onended = null;
          responseAudio.onerror = null;
          stopAvatarSpeechAnimation();
          if (currentAudioObjectUrl) {
            URL.revokeObjectURL(currentAudioObjectUrl);
            currentAudioObjectUrl = null;
          }
          resolve();
        };

        const fail = err => {
          if (finished) return;
          finished = true;
          responseAudio.onended = null;
          responseAudio.onerror = null;
          stopAvatarSpeechAnimation();
          if (currentAudioObjectUrl) {
            URL.revokeObjectURL(currentAudioObjectUrl);
            currentAudioObjectUrl = null;
          }
          reject(err);
        };

        responseAudio.onended = cleanup;
        responseAudio.onerror = () => fail(new Error('Audio playback failed'));

        responseAudio.play().catch(err => {
          fail(err);
        });
      });
    }

    async function speakWithSpeechSynthesis(text) {
      if (!('speechSynthesis' in window) || !text) {
        return;
      }
      return new Promise(resolve => {
        try {
          speechSynthesis.cancel();
          const utterance = new SpeechSynthesisUtterance(text);
          let animationController = null;
          const emphasisRanges = computeEmphasisRanges(text);
          utterance.onstart = () => {
            if (audioContext && audioContext.state === 'suspended') {
              audioContext.resume().catch(() => {});
            }
            animationController = startAvatarSpeechAnimation();
          };
          const finish = () => {
            stopAvatarSpeechAnimation();
            resolve();
          };
          utterance.onboundary = event => {
            if (!animationController || event.name !== 'word') return;
            const emphasised = emphasisRanges.some(range => event.charIndex >= range.start && event.charIndex < range.end);
            animationController.setManualEmphasis(emphasised, emphasised ? 420 : 120);
          };
          utterance.onend = finish;
          utterance.onerror = finish;
          speechSynthesis.speak(utterance);
        } catch (err) {
          console.warn('Speech synthesis unavailable', err);
          stopAvatarSpeechAnimation();
          resolve();
        }
      });
    }

    function base64ToArrayBuffer(base64) {
      const binary = atob(base64);
      const len = binary.length;
      const bytes = new Uint8Array(len);
      for (let i = 0; i < len; i++) {
        bytes[i] = binary.charCodeAt(i);
      }
      return bytes.buffer;
    }

    function blobToBase64(blob) {
      return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onloadend = () => {
