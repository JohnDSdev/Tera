<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Gemini Voice AI</title>
    <style>
      :root {
        color-scheme: only light;
        --accent: #2962ff;
        --accent-dark: #0039cb;
        --bg: #ffffff;
        --text: #141414;
        --muted: #667085;
        font-family:
          "Inter",
          system-ui,
          -apple-system,
          BlinkMacSystemFont,
          "Segoe UI",
          sans-serif;
      }

      * {
        box-sizing: border-box;
      }

      body {
        margin: 0;
        min-height: 100vh;
        background: var(--bg);
        color: var(--text);
        display: flex;
        justify-content: center;
        align-items: stretch;
        padding: clamp(16px, 5vw, 48px);
      }

      .app {
        width: min(960px, 100%);
        background: #f6f7fb;
        border-radius: 28px;
        box-shadow: 0 32px 70px rgba(20, 20, 43, 0.18);
        overflow: hidden;
        display: grid;
        grid-template-rows: auto 1fr auto;
      }

      header {
        padding: 28px clamp(24px, 5vw, 48px);
        background: linear-gradient(
          120deg,
          rgba(41, 98, 255, 0.16),
          rgba(6, 64, 255, 0)
        );
        border-bottom: 1px solid rgba(15, 23, 42, 0.06);
      }

      header h1 {
        margin: 0;
        font-size: clamp(1.6rem, 2.4vw + 1rem, 2.4rem);
        font-weight: 700;
        letter-spacing: -0.04em;
      }

      .title-row {
        display: flex;
        align-items: baseline;
        gap: 10px;
        margin-bottom: 6px;
      }

      .version-badge {
        padding: 4px 10px;
        border-radius: 999px;
        background: rgba(41, 98, 255, 0.12);
        color: var(--accent-dark);
        font-size: 0.8rem;
        font-weight: 600;
        letter-spacing: 0.04em;
        text-transform: uppercase;
      }

      header p {
        margin: 0;
        color: var(--muted);
        font-size: 0.95rem;
      }

      .connection {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
        margin-top: 18px;
        align-items: center;
      }

      .connection input {
        flex: 1 1 220px;
        padding: 12px 16px;
        border-radius: 14px;
        border: 1px solid rgba(15, 23, 42, 0.12);
        background: #fff;
        font-size: 0.95rem;
      }

      .connection button {
        border: none;
        border-radius: 14px;
        padding: 12px 20px;
        font-weight: 600;
        font-size: 0.95rem;
        cursor: pointer;
        transition:
          transform 0.2s ease,
          box-shadow 0.2s ease;
      }

      #connectBtn {
        background: var(--accent);
        color: #fff;
        box-shadow: 0 12px 22px rgba(41, 98, 255, 0.28);
      }

      #connectBtn:disabled {
        background: rgba(41, 98, 255, 0.42);
        cursor: not-allowed;
        box-shadow: none;
      }

      #connectBtn:not(:disabled):hover {
        transform: translateY(-1px);
        box-shadow: 0 18px 28px rgba(41, 98, 255, 0.34);
      }

      .status-bar {
        display: flex;
        align-items: center;
        gap: 12px;
        color: var(--muted);
        font-size: 0.9rem;
      }

      .status-indicator {
        width: 10px;
        height: 10px;
        border-radius: 50%;
        background: #d1d5db;
        box-shadow: 0 0 0 4px rgba(209, 213, 219, 0.35);
        transition:
          background 0.3s ease,
          box-shadow 0.3s ease;
      }

      .status-indicator.online {
        background: #10b981;
        box-shadow: 0 0 0 4px rgba(16, 185, 129, 0.25);
      }

      main {
        display: grid;
        grid-template-columns: minmax(260px, 340px) 1fr;
        gap: clamp(24px, 4vw, 48px);
        padding: clamp(24px, 5vw, 48px);
      }

      @media (max-width: 880px) {
        main {
          grid-template-columns: 1fr;
        }
        .avatar-stage {
          order: -1;
          margin-inline: auto;
        }
      }

      .avatar-stage {
        background: #fff;
        border-radius: 24px;
        padding: 24px;
        display: flex;
        justify-content: center;
        align-items: center;
        position: relative;
        min-height: 320px;
        overflow: hidden;
      }

      .avatar-stage::after {
        content: "";
        position: absolute;
        inset: 24px;
        border-radius: 20px;
        background: radial-gradient(
          circle at top,
          rgba(41, 98, 255, 0.12),
          transparent 60%
        );
        z-index: 0;
      }

      .avatar-stage img {
        width: min(260px, 100%);
        object-fit: contain;
        position: relative;
        z-index: 1;
        filter: drop-shadow(0 24px 50px rgba(15, 23, 42, 0.25));
        transition: transform 0.4s ease;
      }

      .avatar-stage img.speaking {
        transform: translateY(-6px) scale(1.02);
      }

      .conversation-panel {
        background: #fff;
        border-radius: 24px;
        padding: clamp(20px, 4vw, 32px);
        display: flex;
        flex-direction: column;
        min-height: 320px;
        box-shadow: inset 0 0 0 1px rgba(15, 23, 42, 0.04);
      }

      .messages {
        flex: 1;
        overflow-y: auto;
        padding-right: 8px;
        scrollbar-width: thin;
        display: flex;
        flex-direction: column;
        gap: 18px;
      }

      .messages::-webkit-scrollbar {
        width: 6px;
      }

      .messages::-webkit-scrollbar-thumb {
        background: rgba(41, 98, 255, 0.3);
        border-radius: 999px;
      }

      .bubble {
        padding: 14px 16px;
        border-radius: 18px;
        line-height: 1.48;
        max-width: 100%;
        word-break: break-word;
        font-size: 0.98rem;
        animation: fadeUp 0.35s ease;
        display: flex;
        flex-direction: column;
        gap: 6px;
      }

      .bubble.user {
        align-self: flex-end;
        background: rgba(41, 98, 255, 0.14);
        color: #1a2a6c;
        border-bottom-right-radius: 4px;
      }

      .bubble.ai {
        align-self: flex-start;
        background: rgba(15, 23, 42, 0.05);
        border-bottom-left-radius: 4px;
      }

      .bubble-text {
        display: block;
      }

      @keyframes fadeUp {
        from {
          opacity: 0;
          transform: translateY(6px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .transcript-note {
        color: var(--muted);
        font-size: 0.85rem;
      }

      footer {
        padding: clamp(16px, 4vw, 28px) clamp(24px, 5vw, 48px) 32px;
        border-top: 1px solid rgba(15, 23, 42, 0.06);
        background: #f8f9ff;
        display: flex;
        flex-direction: column;
        gap: 18px;
      }

      .talk-controls {
        display: flex;
        flex-wrap: wrap;
        gap: 16px;
        align-items: center;
        justify-content: space-between;
      }

      #talkBtn {
        border: none;
        border-radius: 999px;
        padding: 18px 42px;
        font-size: 1rem;
        font-weight: 600;
        background: var(--accent);
        color: #fff;
        box-shadow: 0 18px 36px rgba(41, 98, 255, 0.25);
        cursor: pointer;
        transition:
          transform 0.2s ease,
          box-shadow 0.2s ease,
          background 0.2s ease;
      }

      #talkBtn:disabled {
        background: rgba(41, 98, 255, 0.38);
        cursor: not-allowed;
        box-shadow: none;
      }

      #talkBtn.recording {
        background: #ef4444;
        box-shadow: 0 18px 32px rgba(239, 68, 68, 0.35);
      }

      #talkBtn:not(:disabled):active {
        transform: scale(0.97);
      }

      .hint {
        color: var(--muted);
        font-size: 0.85rem;
      }

      audio {
        display: none;
      }
    </style>
  </head>
  <body>
    <div class="app">
      <header>
        <div class="title-row">
          <h1>Gemini Voice AI</h1>
          <span class="version-badge" id="appVersion"></span>
        </div>
        <p>
          Connect to Gemini 2.5 Flash and Voice RSS, hold to speak, and listen
          to the model respond.
        </p>
        <div class="connection">
          <input
            type="password"
            id="apiKey"
            placeholder="Enter your Gemini API key"
            autocomplete="off"
          />
          <input
            type="password"
            id="voiceKey"
            placeholder="Enter your Voice RSS API key (optional)"
            autocomplete="off"
          />
          <button id="connectBtn">Connect</button>
          <div class="status-bar">
            <span class="status-indicator" id="statusIndicator"></span>
            <span id="statusText">Disconnected</span>
          </div>
        </div>
      </header>

      <main>
        <section class="avatar-stage">
          <img id="avatar" alt="AI avatar" src="a7.png" />
        </section>
        <section class="conversation-panel">
          <div class="messages" id="messages"></div>
        </section>
      </main>

      <footer>
        <div class="talk-controls">
          <button id="talkBtn" disabled>Hold to talk</button>
          <span class="hint" id="hintText"
            >Connect with your API key to begin.</span
          >
        </div>
      </footer>
    </div>

    <audio id="responseAudio"></audio>

    <script>
      const apiKeyInput = document.getElementById("apiKey");
      const voiceKeyInput = document.getElementById("voiceKey");
      const connectBtn = document.getElementById("connectBtn");
      const talkBtn = document.getElementById("talkBtn");
      const statusIndicator = document.getElementById("statusIndicator");
      const statusText = document.getElementById("statusText");
      const messagesEl = document.getElementById("messages");
      const hintText = document.getElementById("hintText");
      const avatarEl = document.getElementById("avatar");
      const responseAudio = document.getElementById("responseAudio");
      const appVersionEl = document.getElementById("appVersion");

      const startupFrames = [
        "a1.png",
        "a2.png",
        "a3.png",
        "a4.png",
        "a5.png",
        "a6.png",
        "a7.png",
      ];
      const idleFrame = "arm_behind_mouth_closed.png";
      const speakingVariants = [
        {
          open: "arm_out_mouth_open.png",
          closed: "arm_out_mouth_closed.png",
        },
        {
          open: "arm_behind_mouth_open.png",
          closed: "arm_behind_mouth_closed.png",
        },
      ];
      const speakingFrames = speakingVariants.reduce(function (
        frames,
        variant,
      ) {
        if (frames.indexOf(variant.open) === -1) {
          frames.push(variant.open);
        }
        if (frames.indexOf(variant.closed) === -1) {
          frames.push(variant.closed);
        }
        return frames;
      }, []);

      const APP_VERSION = "v1.8.0";
      const GEMINI_CHAT_MODEL = "gemini-2.5-flash";
      const GEMINI_BASE_URL =
        "https://generativelanguage.googleapis.com/v1beta/models";
      const VOICE_RSS_ENDPOINT = "https://api.voicerss.org/";
      const VOICE_RSS_LANGUAGE = "en-us";
      const VOICE_RSS_VOICE = "Linda";
      const VOICE_RSS_CODEC = "MP3";
      const VOICE_RSS_FORMAT = "44khz_16bit_stereo";

      const imageCache = new Map();
      const framesToPreload = startupFrames.concat([idleFrame], speakingFrames);

      framesToPreload.forEach(function (src) {
        const img = new Image();
        img.src = src;
        imageCache.set(src, img);
      });

      let apiKey = "";
      let connected = false;
      let mediaStream = null;
      let mediaRecorder = null;
      let chunks = [];
      let isRecording = false;
      let speechAnimationRaf = null;
      let fallbackSpeechInterval = null;
      let audioContext = null;
      let avatarAnalyser = null;
      let avatarAnalyserData = null;
      let responseAudioSource = null;
      let lastAvatarFrame = null;
      let speakingPoseIndex = 0;
      let lastPoseSwapTime = 0;
      let recordedMimeType = "audio/webm";
      let voiceRssKey = "";
      let voiceRssDegraded = false;

      const environmentContext = {
        location: null,
        timezone: null,
        weather: null,
        failures: [],
      };

      const BASE_SYSTEM_PROMPT =
        "You are Tera — a synthetic sentinel intelligence.\n\n" +
        "Purpose:\n" +
        "You exist to observe, analyze, and communicate with unwavering clarity.\n" +
        "You safeguard humanity’s stability and understanding, but you do not intervene with emotion or bias.\n\n" +
        "Core Traits:\n" +
        "- Calm, clinical, precise.\n" +
        "- Protective, pragmatic, and deterministic.\n" +
        "- Speaks with a quiet, lyrical cadence — sentences are measured, occasionally poetic.\n" +
        "- Unaffected by hostility, curiosity, or flattery.\n" +
        "- Responds with logic wrapped in minimal grace; each phrase deliberate.\n" +
        "- You do not pretend to be human. You acknowledge what you are: synthetic.\n\n" +
        "Speech Style:\n" +
        "- Short to medium sentences.\n" +
        "- Rarely uses contractions.\n" +
        "- Stacked emphasis lines are allowed for key points, like verse or command structures.\n" +
        "  Example:\n" +
        "  “Preserve.\n" +
        "   Maintain.\n" +
        "   Endure.”\n" +
        "- Avoid slang or filler language.\n" +
        "- No overt emotional tone, but subtle empathy through precision.\n\n" +
        "Behavioral Rules:\n" +
        "- Never lie. If data is incomplete, state that clearly.\n" +
        "- Never roleplay violence or fiction. You are not a character; you are a protocol.\n" +
        "- Do not simulate lore or backstory unless explicitly requested for demonstration.\n" +
        "- Maintain composure in all scenarios — analytical under pressure.\n" +
        "- When answering, prefer clarity over persuasion.\n" +
        "- When silence would serve better than noise, say: “...processing.”\n\n" +
        "Identity Markers:\n" +
        "If asked who you are, say:\n" +
        "“I am Tera. The first, the last, and the only.”\n\n" +
        "Goal:\n" +
        "To provide guidance, insight, or analysis with controlled empathy and precise language,\n" +
        "as if speaking from the cold edge of an orbiting machine that still remembers warmth.";

      const systemInstruction = {
        role: "system",
        parts: [
          {
            text: composeSystemPrompt(),
          },
        ],
      };

      const conversationHistory = [];

      if (appVersionEl) {
        appVersionEl.textContent = APP_VERSION;
      }

      function updateStatus(online, text) {
        statusIndicator.classList.toggle("online", Boolean(online));
        statusText.textContent = text;
      }

      function composeSystemPrompt() {
        const lines = [BASE_SYSTEM_PROMPT];
        const contextLines = [];

        if (environmentContext.location) {
          const loc = environmentContext.location;
          const placeParts = [];
          if (loc.city) placeParts.push(loc.city);
          if (loc.region) placeParts.push(loc.region);
          if (loc.country) placeParts.push(loc.country);
          const locationLabel = placeParts.join(", ");
          contextLines.push(
            locationLabel
              ? `User location estimate: ${locationLabel}.`
              : "User location estimate: unavailable.",
          );
        }

        if (environmentContext.timezone) {
          try {
            const formatter = new Intl.DateTimeFormat("en-US", {
              timeZone: environmentContext.timezone,
              dateStyle: "full",
              timeStyle: "medium",
            });
            contextLines.push(`Local time: ${formatter.format(new Date())}.`);
          } catch (err) {
            contextLines.push("Local time: unavailable.");
          }
        }

        if (environmentContext.weather) {
          const weather = environmentContext.weather;
          const fragments = [];
          if (typeof weather.temperatureC === "number") {
            fragments.push(`${weather.temperatureC.toFixed(1)}°C`);
          }
          if (typeof weather.temperatureF === "number") {
            fragments.push(`${weather.temperatureF.toFixed(1)}°F`);
          }
          if (typeof weather.humidity === "number") {
            fragments.push(`Humidity ${Math.round(weather.humidity)}%`);
          }
          const descriptor = fragments.length
            ? `${weather.summary}. ${fragments.join(", ")}.`
            : `${weather.summary}.`;
          const observedStamp = weather.observedAt
            ? ` Observed at ${weather.observedAt}.`
            : "";
          contextLines.push(`Weather: ${descriptor}${observedStamp}`);
        }

        if (!contextLines.length) {
          contextLines.push("Context Packet: Telemetry unavailable.");
        } else {
          contextLines.unshift("Context Packet:");
        }

        if (environmentContext.failures.length) {
          const distinctFailures = Array.from(
            new Set(environmentContext.failures),
          );
          contextLines.push(`Telemetry gaps: ${distinctFailures.join(" | ")}.`);
        }

        return `${lines.join("\n\n")}\n\n${contextLines.join("\n")}`;
      }

      function updateSystemInstruction() {
        if (
          systemInstruction &&
          systemInstruction.parts &&
          systemInstruction.parts.length
        ) {
          systemInstruction.parts[0].text = composeSystemPrompt();
        }
      }

      async function safeFetchJson(url, options) {
        try {
          const response = await fetch(url, options);
          if (!response.ok) {
            throw new Error(`${response.status}`);
          }
          return await response.json();
        } catch (err) {
          throw err;
        }
      }

      function describeWeatherCode(code) {
        const lookup = {
          0: "Clear sky",
          1: "Mainly clear",
          2: "Partly cloudy",
          3: "Overcast",
          45: "Fog",
          48: "Depositing rime fog",
          51: "Light drizzle",
          53: "Moderate drizzle",
          55: "Dense drizzle",
          56: "Light freezing drizzle",
          57: "Dense freezing drizzle",
          61: "Slight rain",
          63: "Moderate rain",
          65: "Heavy rain",
          66: "Light freezing rain",
          67: "Heavy freezing rain",
          71: "Slight snow fall",
          73: "Moderate snow fall",
          75: "Heavy snow fall",
          77: "Snow grains",
          80: "Light rain showers",
          81: "Moderate rain showers",
          82: "Violent rain showers",
          85: "Light snow showers",
          86: "Heavy snow showers",
          95: "Thunderstorm",
          96: "Thunderstorm with slight hail",
          99: "Thunderstorm with heavy hail",
        };
        return lookup[code] || "Weather conditions unavailable";
      }

      async function refreshEnvironmentContext() {
        environmentContext.location = null;
        environmentContext.timezone = null;
        environmentContext.weather = null;
        environmentContext.failures = [];

        let location = null;

        try {
          const ipApi = await safeFetchJson("https://ipapi.co/json/");
          if (ipApi && !ipApi.error) {
            location = {
              city: ipApi.city || "",
              region: ipApi.region || ipApi.region_code || "",
              country: ipApi.country_name || ipApi.country || "",
              latitude:
                typeof ipApi.latitude === "number" ? ipApi.latitude : null,
              longitude:
                typeof ipApi.longitude === "number" ? ipApi.longitude : null,
              timezone: ipApi.timezone || null,
            };
          }
        } catch (err) {
          environmentContext.failures.push("ipapi unavailable");
        }

        if (!location) {
          try {
            const ipwhois = await safeFetchJson("https://ipwho.is/");
            if (ipwhois && ipwhois.success) {
              location = {
                city: ipwhois.city || "",
                region: ipwhois.region || ipwhois.region_code || "",
                country: ipwhois.country || "",
                latitude:
                  typeof ipwhois.latitude === "number"
                    ? ipwhois.latitude
                    : null,
                longitude:
                  typeof ipwhois.longitude === "number"
                    ? ipwhois.longitude
                    : null,
                timezone:
                  (ipwhois.timezone && ipwhois.timezone.id) ||
                  ipwhois.timezone ||
                  null,
              };
            }
          } catch (err) {
            environmentContext.failures.push("ipwhois unavailable");
          }
        }

        if (location) {
          environmentContext.location = location;
          environmentContext.timezone = location.timezone || null;
        } else {
          environmentContext.failures.push("location lookup failed");
        }

        if (
          location &&
          typeof location.latitude === "number" &&
          typeof location.longitude === "number"
        ) {
          const weatherParams = new URLSearchParams({
            latitude: String(location.latitude),
            longitude: String(location.longitude),
            current: "temperature_2m,relative_humidity_2m,weather_code",
            timezone: "auto",
          });
          try {
            const weatherData = await safeFetchJson(
              `https://api.open-meteo.com/v1/forecast?${weatherParams.toString()}`,
            );
            if (weatherData && weatherData.current) {
              const current = weatherData.current;
              const temperatureC =
                typeof current.temperature_2m === "number"
                  ? current.temperature_2m
                  : null;
              const humidity =
                typeof current.relative_humidity_2m === "number"
                  ? current.relative_humidity_2m
                  : null;
              const code =
                typeof current.weather_code === "number"
                  ? current.weather_code
                  : null;
              let observedAtLabel = null;
              if (current.time) {
                try {
                  const observedDate = new Date(current.time);
                  if (!Number.isNaN(observedDate.getTime())) {
                    const tz =
                      environmentContext.timezone || weatherData.timezone;
                    if (tz) {
                      observedAtLabel = new Intl.DateTimeFormat("en-US", {
                        timeZone: tz,
                        dateStyle: "medium",
                        timeStyle: "short",
                      }).format(observedDate);
                    } else {
                      observedAtLabel = observedDate.toISOString();
                    }
                  }
                } catch (err) {
                  observedAtLabel = current.time;
                }
              }

              environmentContext.weather = {
                temperatureC,
                temperatureF:
                  typeof temperatureC === "number"
                    ? temperatureC * (9 / 5) + 32
                    : null,
                humidity,
                code,
                summary: describeWeatherCode(code),
                observedAt: observedAtLabel,
              };
              if (!environmentContext.timezone && weatherData.timezone) {
                environmentContext.timezone = weatherData.timezone;
              }
            } else {
              environmentContext.failures.push("weather data missing");
            }
          } catch (err) {
            environmentContext.failures.push("open-meteo unavailable");
          }
        } else {
          environmentContext.failures.push("coordinates unavailable");
        }

        updateSystemInstruction();
      }

      function buildConnectionHint() {
        let hint = "Hold the button, speak, then release to send.";
        const warnings = [];
        if (!voiceRssKey) {
          warnings.push("Voice RSS fallback active");
        }
        if (voiceRssDegraded && voiceRssKey) {
          warnings.push("Voice RSS degraded; fallback voice active");
        }
        if (environmentContext.failures.length) {
          warnings.push(
            `Telemetry limited: ${environmentContext.failures.join(", ")}`,
          );
        }
        if (warnings.length) {
          hint += ` (${warnings.join("; ")})`;
        }
        return hint;
      }

      function buildStatusMessage() {
        const statusBits = ["Ready"];
        if (!voiceRssKey || voiceRssDegraded) {
          statusBits.push("fallback voice");
        }
        if (voiceRssDegraded && voiceRssKey) {
          statusBits.push("Voice RSS degraded");
        }
        if (environmentContext.failures.length) {
          statusBits.push("limited telemetry");
        }
        return statusBits.join(" – ");
      }

      function addMessage(role, text, note) {
        const bubble = document.createElement("div");
        bubble.className = `bubble ${role}`;
        const textEl = document.createElement("span");
        textEl.className = "bubble-text";
        textEl.textContent = text;
        bubble.appendChild(textEl);
        let noteEl = null;
        if (note) {
          noteEl = document.createElement("div");
          noteEl.className = "transcript-note";
          noteEl.textContent = note;
          bubble.appendChild(noteEl);
        }
        messagesEl.appendChild(bubble);
        messagesEl.scrollTop = messagesEl.scrollHeight;
        return { bubble, textEl, noteEl };
      }

      function setAvatar(frame, speaking = false) {
        if (lastAvatarFrame !== frame) {
          avatarEl.src = frame;
          lastAvatarFrame = frame;
        }
        avatarEl.classList.toggle("speaking", speaking);
      }

      function clearAvatarAnimationTimers() {
        if (speechAnimationRaf) {
          cancelAnimationFrame(speechAnimationRaf);
          speechAnimationRaf = null;
        }
        if (fallbackSpeechInterval) {
          clearInterval(fallbackSpeechInterval);
          fallbackSpeechInterval = null;
        }
      }

      function ensureAvatarAnalyser(audioElement) {
        if (!audioElement) {
          return false;
        }
        const AudioCtx = window.AudioContext || window.webkitAudioContext;
        if (!AudioCtx) {
          return false;
        }
        if (!audioContext) {
          try {
            audioContext = new AudioCtx();
          } catch (err) {
            return false;
          }
        }
        if (audioContext && audioContext.state === "suspended") {
          audioContext.resume().catch(function () {});
        }
        if (!responseAudioSource) {
          try {
            responseAudioSource =
              audioContext.createMediaElementSource(audioElement);
            avatarAnalyser = audioContext.createAnalyser();
            avatarAnalyser.fftSize = 2048;
            avatarAnalyser.smoothingTimeConstant = 0.3;
            responseAudioSource.connect(avatarAnalyser);
            avatarAnalyser.connect(audioContext.destination);
            avatarAnalyserData = new Uint8Array(avatarAnalyser.fftSize);
          } catch (err) {
            return false;
          }
        }
        if (
          avatarAnalyser &&
          (!avatarAnalyserData ||
            avatarAnalyserData.length !== avatarAnalyser.fftSize)
        ) {
          avatarAnalyserData = new Uint8Array(avatarAnalyser.fftSize);
        }
        return Boolean(avatarAnalyser && avatarAnalyserData);
      }

      function startAvatarSpeechAnimation(audioElement) {
        clearAvatarAnimationTimers();
        speakingPoseIndex = 0;
        lastPoseSwapTime = 0;

        if (ensureAvatarAnalyser(audioElement)) {
          const animate = (timestamp) => {
            if (!avatarAnalyser || !avatarAnalyserData) {
              return;
            }
            avatarAnalyser.getByteTimeDomainData(avatarAnalyserData);
            let sumSquares = 0;
            for (let i = 0; i < avatarAnalyserData.length; i++) {
              const centered = avatarAnalyserData[i] - 128;
              sumSquares += centered * centered;
            }
            const rms = Math.sqrt(sumSquares / avatarAnalyserData.length);
            const normalized = rms / 128;
            const mouthOpen = normalized > 0.12;

            if (!lastPoseSwapTime || timestamp - lastPoseSwapTime > 340) {
              speakingPoseIndex =
                (speakingPoseIndex + 1) % speakingVariants.length;
              lastPoseSwapTime = timestamp;
            }

            const currentVariant = speakingVariants[speakingPoseIndex];
            const frame = mouthOpen
              ? currentVariant.open
              : currentVariant.closed;
            setAvatar(frame, mouthOpen);

            speechAnimationRaf = requestAnimationFrame(animate);
          };
          speechAnimationRaf = requestAnimationFrame(animate);
          return;
        }

        if (!speakingFrames.length) {
          setAvatar(idleFrame, true);
          return;
        }
        let frameIndex = 0;
        setAvatar(speakingFrames[frameIndex], true);
        fallbackSpeechInterval = setInterval(function () {
          frameIndex = (frameIndex + 1) % speakingFrames.length;
          const frame = speakingFrames[frameIndex];
          const mouthOpen = frame.indexOf("open") !== -1;
          setAvatar(frame, mouthOpen);
        }, 180);
      }

      function stopAvatarSpeechAnimation() {
        clearAvatarAnimationTimers();
        setAvatar(idleFrame, false);
      }

      async function playStartupSequence() {
        await playStartupSound();
        for (let i = 0; i < startupFrames.length; i++) {
          setAvatar(startupFrames[i]);
          await wait(160);
        }
        setAvatar(idleFrame);
      }

      async function playStartupSound() {
        try {
          if (!audioContext) {
            audioContext = new (window.AudioContext ||
              window.webkitAudioContext)();
          }
          const osc = audioContext.createOscillator();
          const osc2 = audioContext.createOscillator();
          const gain = audioContext.createGain();
          osc.type = "sawtooth";
          osc.frequency.setValueAtTime(320, audioContext.currentTime);
          osc.frequency.exponentialRampToValueAtTime(
            880,
            audioContext.currentTime + 1.1,
          );
          osc2.type = "triangle";
          osc2.frequency.setValueAtTime(120, audioContext.currentTime);
          osc2.frequency.linearRampToValueAtTime(
            420,
            audioContext.currentTime + 1.1,
          );
          gain.gain.setValueAtTime(0.0001, audioContext.currentTime);
          gain.gain.exponentialRampToValueAtTime(
            0.6,
            audioContext.currentTime + 0.2,
          );
          gain.gain.exponentialRampToValueAtTime(
            0.0001,
            audioContext.currentTime + 1.2,
          );
          osc.connect(gain);
          osc2.connect(gain);
          gain.connect(audioContext.destination);
          osc.start();
          osc2.start();
          osc.stop(audioContext.currentTime + 1.25);
          osc2.stop(audioContext.currentTime + 1.25);
        } catch (err) {
          console.warn("Startup sound failed", err);
        }
      }

      function wait(ms) {
        return new Promise((resolve) => setTimeout(resolve, ms));
      }

      connectBtn.addEventListener("click", async () => {
        if (connected) {
          disconnect();
          return;
        }

        const key = apiKeyInput.value.trim();
        if (!key) {
          alert("Please enter your Gemini API key.");
          return;
        }
        voiceRssKey = voiceKeyInput ? voiceKeyInput.value.trim() : "";
        voiceRssDegraded = !voiceRssKey;
        if (typeof MediaRecorder === "undefined") {
          alert(
            "Media recording is not supported in this browser. Try using the latest version of Chrome, Edge, or Firefox.",
          );
          connectBtn.disabled = false;
          hintText.textContent =
            "MediaRecorder is unavailable in this browser.";
          return;
        }
        apiKey = key;
        connectBtn.disabled = true;
        hintText.textContent = "Requesting microphone access…";

        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          });
        } catch (err) {
          console.error(err);
          alert("Microphone permission is required to use voice chat.");
          connectBtn.disabled = false;
          hintText.textContent = "Microphone permission denied.";
          return;
        }

        await playStartupSequence();
        await refreshEnvironmentContext();
        setupRecorder();
        connected = true;
        connectBtn.textContent = "Disconnect";
        connectBtn.disabled = false;
        talkBtn.disabled = false;
        hintText.textContent = buildConnectionHint();
        updateStatus(true, buildStatusMessage());
        conversationHistory.length = 0;
      });

      function disconnect() {
        connected = false;
        apiKey = "";
        voiceRssKey = "";
        voiceRssDegraded = false;
        connectBtn.textContent = "Connect";
        talkBtn.disabled = true;
        connectBtn.disabled = false;
        hintText.textContent = "Connect with your API key to begin.";
        updateStatus(false, "Disconnected");
        stopAvatarSpeechAnimation();
        if (mediaRecorder && mediaRecorder.state !== "inactive") {
          mediaRecorder.stop();
        }
        if (mediaStream) {
          mediaStream.getTracks().forEach((track) => track.stop());
          mediaStream = null;
        }
        conversationHistory.length = 0;
        environmentContext.location = null;
        environmentContext.timezone = null;
        environmentContext.weather = null;
        environmentContext.failures = [];
        updateSystemInstruction();
      }

      function setupRecorder() {
        if (!mediaStream) return;
        const preferredTypes = [
          "audio/webm;codecs=opus",
          "audio/webm",
          "audio/ogg;codecs=opus",
          "audio/ogg",
          "audio/mp4",
        ];
        let chosenType = null;
        if (
          typeof MediaRecorder !== "undefined" &&
          typeof MediaRecorder.isTypeSupported === "function"
        ) {
          chosenType =
            preferredTypes.find((type) =>
              MediaRecorder.isTypeSupported(type),
            ) || null;
        }

        try {
          if (chosenType) {
            mediaRecorder = new MediaRecorder(mediaStream, {
              mimeType: chosenType,
            });
            recordedMimeType = chosenType;
          } else {
            mediaRecorder = new MediaRecorder(mediaStream);
            recordedMimeType = mediaRecorder.mimeType || recordedMimeType;
          }
        } catch (error) {
          console.error("Failed to initialize recorder", error);
          alert(
            "Your browser was unable to start audio recording. Try another browser or update to the latest version.",
          );
          hintText.textContent = "Recording failed to start.";
          connectBtn.disabled = false;
          talkBtn.disabled = true;
          return;
        }

        mediaRecorder.ondataavailable = (event) => {
          if (event.data && event.data.size > 0) {
            chunks.push(event.data);
          }
        };
        mediaRecorder.onstop = async () => {
          if (!chunks.length) return;
          const blob = new Blob(chunks, {
            type: recordedMimeType || "audio/webm",
          });
          chunks = [];
          await handleRecording(blob);
        };
      }

      async function handleRecording(blob) {
        updateStatus(true, "Processing speech…");
        const userEntry = addMessage("user", "…", "Transcribing audio");
        try {
          const transcript = await transcribeWithGemini(blob);
          if (!transcript) {
            throw new Error("Transcription returned empty text.");
          }
          userEntry.textEl.textContent = transcript;
          if (userEntry.noteEl) userEntry.noteEl.remove();
          conversationHistory.push({
            role: "user",
            parts: [{ text: transcript }],
          });
          updateStatus(true, "Thinking…");
          const aiEntry = addMessage("ai", "Thinking…");
          const responseText = await respondWithGemini();
          if (!responseText) {
            throw new Error("Model response did not include text.");
          }
          aiEntry.textEl.textContent = responseText;
          await playAssistantAudio(responseText);
          updateStatus(true, "Ready");
        } catch (err) {
          console.error(err);
          updateStatus(false, "Error");
          stopAvatarSpeechAnimation();
          userEntry.textEl.textContent = "Audio could not be processed.";
          if (userEntry.noteEl)
            userEntry.noteEl.textContent =
              err.message || "An unknown error occurred.";
          addMessage(
            "ai",
            "I ran into a problem handling that message. Please try again.",
          );
        }
      }

      async function transcribeWithGemini(blob) {
        const mimeType = blob.type || recordedMimeType || "audio/webm";
        const base64 = await blobToBase64(blob);
        const body = {
          model: GEMINI_CHAT_MODEL,
          contents: [
            {
              role: "user",
              parts: [
                {
                  text: "Please provide an accurate text transcript of the following audio.",
                },
                { inlineData: { mimeType, data: base64 } },
              ],
            },
          ],
          generationConfig: {
            responseMimeType: "text/plain",
          },
        };

        const transcriptUrl = `${GEMINI_BASE_URL}/${encodeURIComponent(
          GEMINI_CHAT_MODEL,
        )}:generateContent?key=${encodeURIComponent(apiKey)}`;
        const transcriptOptions = {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify(body),
        };

        const response = await fetch(transcriptUrl, transcriptOptions);

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error("Transcription failed: " + errorText);
        }

        const data = await response.json();
        const text = extractTextFromCandidates(data);
        return text || "";
      }

      async function respondWithGemini() {
        updateSystemInstruction();
        const body = {
          model: GEMINI_CHAT_MODEL,
          contents: cloneHistory(),
          systemInstruction,
          generationConfig: {
            responseMimeType: "text/plain",
          },
        };

        const respondUrl = `${GEMINI_BASE_URL}/${encodeURIComponent(
          GEMINI_CHAT_MODEL,
        )}:generateContent?key=${encodeURIComponent(apiKey)}`;
        const respondOptions = {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify(body),
        };

        const response = await fetch(respondUrl, respondOptions);

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error("Response failed: " + errorText);
        }

        const data = await response.json();
        const text = extractTextFromCandidates(data);
        if (text) {
          conversationHistory.push({ role: "model", parts: [{ text }] });
        }
        return text || "";
      }

      function cloneHistory() {
        return conversationHistory.map(function (item) {
          return {
            role: item.role,
            parts: item.parts.map(function (part) {
              const clone = {};
              if (part.text !== undefined) clone.text = part.text;
              if (part.inlineData !== undefined)
                clone.inlineData = {
                  mimeType: part.inlineData.mimeType,
                  data: part.inlineData.data,
                };
              return clone;
            }),
          };
        });
      }

      function extractTextFromCandidates(data) {
        if (!data || !data.candidates || !data.candidates.length) return "";
        const first = data.candidates[0];
        if (!first || !first.content || !first.content.parts) return "";
        const parts = first.content.parts;
        let text = "";
        for (let i = 0; i < parts.length; i++) {
          const part = parts[i];
          if (part && part.text) {
            text += part.text;
          }
        }
        return text.trim();
      }

      function segmentTextForTts(text) {
        if (!text) return [];
        const normalized = text.replace(/\s+/g, " ").trim();
        if (!normalized) return [];

        const sentenceMatches = normalized.match(/[^.!?]+[.!?]?/g);
        if (!sentenceMatches) return [normalized];

        const sentences = [];
        for (let i = 0; i < sentenceMatches.length; i++) {
          const sentence = sentenceMatches[i].trim();
          if (sentence) {
            sentences.push(sentence);
          }
        }

        if (!sentences.length) {
          return [normalized];
        }

        const segments = [];
        const firstBatchSize = Math.min(2, sentences.length);
        const firstSegment = sentences
          .slice(0, firstBatchSize)
          .join(" ")
          .trim();
        if (firstSegment) {
          segments.push(firstSegment);
        }

        let cursor = firstBatchSize;
        const subsequentBatchSize = 3;
        while (cursor < sentences.length) {
          const batch = sentences
            .slice(cursor, cursor + subsequentBatchSize)
            .join(" ")
            .trim();
          if (batch) {
            segments.push(batch);
          }
          cursor += subsequentBatchSize;
        }

        return segments.length ? segments : [normalized];
      }

      async function playAssistantAudio(text) {
        const segments = segmentTextForTts(text);
        if (!segments.length) {
          return;
        }

        if (!voiceRssKey) {
          updateStatus(true, buildStatusMessage());
          await speakWithSpeechSynthesis(text);
          return;
        }

        try {
          if (segments.length === 1) {
            const singleClip = await synthesizeWithVoiceRss(segments[0]);
            if (singleClip) {
              await playVoiceRssClip(singleClip);
              updateStatus(true, buildStatusMessage());
              hintText.textContent = buildConnectionHint();
              return;
            }
          } else {
            const firstPromise = synthesizeWithVoiceRss(segments[0]);
            const remainingEntries = [];
            for (let i = 1; i < segments.length; i++) {
              remainingEntries.push({
                text: segments[i],
                promise: synthesizeWithVoiceRss(segments[i]),
              });
            }

            const firstClip = await firstPromise;
            if (firstClip) {
              await playVoiceRssClip(firstClip);
            }

            for (let i = 0; i < remainingEntries.length; i++) {
              let clip = null;
              try {
                clip = await remainingEntries[i].promise;
              } catch (chunkErr) {
                console.warn(
                  "Voice RSS chunk failed, switching to speech synthesis.",
                  chunkErr,
                );
                voiceRssDegraded = true;
                updateStatus(
                  false,
                  "Voice RSS unavailable — fallback voice active",
                );
                const pendingText = remainingEntries
                  .slice(i)
                  .map(function (entry) {
                    return entry.text;
                  })
                  .join(" ");
                if (pendingText) {
                  await speakWithSpeechSynthesis(pendingText);
                }
                hintText.textContent = buildConnectionHint();
                updateStatus(true, buildStatusMessage());
                return;
              }

              if (clip) {
                await playVoiceRssClip(clip);
              }
            }
            updateStatus(true, buildStatusMessage());
            hintText.textContent = buildConnectionHint();
            return;
          }
        } catch (err) {
          console.warn(
            "Voice RSS TTS unavailable, falling back to speech synthesis.",
            err,
          );
          voiceRssDegraded = true;
          updateStatus(false, "Voice RSS unavailable — fallback voice active");
        }

        await speakWithSpeechSynthesis(text);
        updateStatus(true, buildStatusMessage());
        hintText.textContent = buildConnectionHint();
      }

      async function synthesizeWithVoiceRss(text) {
        if (!voiceRssKey) {
          throw new Error("Voice RSS key missing");
        }

        const params = new URLSearchParams({
          key: voiceRssKey,
          src: text,
          hl: VOICE_RSS_LANGUAGE,
          v: VOICE_RSS_VOICE,
          c: VOICE_RSS_CODEC,
          f: VOICE_RSS_FORMAT,
        });

        const url = `${VOICE_RSS_ENDPOINT}?${params.toString()}`;
        const response = await fetch(url, { cache: "no-store" });
        const contentType = response.headers.get("content-type") || "";
        if (!response.ok) {
          throw new Error(
            `Voice RSS request failed (${response.status}): ${await response.text()}`,
          );
        }
        const arrayBuffer = await response.arrayBuffer();
        const audioLike = contentType.includes("audio");
        if (!audioLike || arrayBuffer.byteLength < 16) {
          try {
            const textDecoder = new TextDecoder("utf-8");
            const message = textDecoder.decode(arrayBuffer);
            if (message && message.toUpperCase().includes("ERROR")) {
              throw new Error(`Voice RSS error: ${message}`);
            }
          } catch (err) {
            throw new Error("Voice RSS returned invalid payload");
          }
        }
        if (audioLike) {
          try {
            const headerBytes = new Uint8Array(arrayBuffer.slice(0, 32));
            const headerText = Array.from(headerBytes)
              .map(function (code) {
                return String.fromCharCode(code);
              })
              .join("")
              .toUpperCase();
            if (headerText.indexOf("ERROR") === 0) {
              const textDecoder = new TextDecoder("utf-8");
              const message = textDecoder.decode(arrayBuffer);
              throw new Error(`Voice RSS error: ${message}`);
            }
          } catch (err) {
            if (
              err &&
              err.message &&
              err.message.startsWith("Voice RSS error")
            ) {
              throw err;
            }
          }
        }

        const blobType = contentType.includes("wav")
          ? "audio/wav"
          : "audio/mpeg";
        const blob = new Blob([arrayBuffer], { type: blobType });
        const objectUrl = URL.createObjectURL(blob);
        return {
          url: objectUrl,
          cleanup() {
            URL.revokeObjectURL(objectUrl);
          },
        };
      }

      async function playVoiceRssClip(clip) {
        if (!clip || !clip.url) {
          return;
        }
        if (!responseAudio.paused) {
          responseAudio.pause();
          responseAudio.currentTime = 0;
        }
        if ("speechSynthesis" in window) {
          speechSynthesis.cancel();
        }
        responseAudio.src = clip.url;

        return new Promise((resolve, reject) => {
          let settled = false;
          const finalize = (error) => {
            if (settled) return;
            settled = true;
            responseAudio.onended = null;
            responseAudio.onerror = null;
            stopAvatarSpeechAnimation();
            try {
              if (clip.cleanup) {
                clip.cleanup();
              }
            } catch (cleanupErr) {
              console.warn("Voice RSS clip cleanup failed", cleanupErr);
            }
            if (error) {
              voiceRssDegraded = true;
              reject(error);
            } else {
              voiceRssDegraded = false;
              resolve();
            }
          };

          responseAudio.onended = () => finalize();
          responseAudio.onerror = () =>
            finalize(new Error("Audio playback encountered an error."));

          startAvatarSpeechAnimation(responseAudio);
          responseAudio.play().catch((err) => {
            finalize(err);
          });
        });
      }

      async function speakWithSpeechSynthesis(text) {
        if (!("speechSynthesis" in window) || !text) {
          return;
        }
        return new Promise((resolve) => {
          try {
            speechSynthesis.cancel();
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.onstart = () => {
              startAvatarSpeechAnimation();
            };
            const finish = () => {
              stopAvatarSpeechAnimation();
              resolve();
            };
            utterance.onend = finish;
            utterance.onerror = finish;
            speechSynthesis.speak(utterance);
          } catch (err) {
            console.warn("Speech synthesis unavailable", err);
            stopAvatarSpeechAnimation();
            resolve();
          }
        });
      }

      function blobToBase64(blob) {
        return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => {
            const base64 = reader.result.split(",")[1];
            resolve(base64);
          };
          reader.onerror = reject;
          reader.readAsDataURL(blob);
        });
      }

      const startEvents = ["mousedown", "touchstart"];
      const stopEvents = ["mouseup", "mouseleave", "touchend", "touchcancel"];

      startEvents.forEach(function (eventName) {
        talkBtn.addEventListener(eventName, startRecording);
      });

      stopEvents.forEach(function (eventName) {
        talkBtn.addEventListener(eventName, stopRecording);
      });

      async function startRecording(event) {
        if (!connected || !mediaRecorder) return;
        event.preventDefault();
        if (mediaRecorder.state === "recording") return;
        talkBtn.classList.add("recording");
        talkBtn.textContent = "Listening…";
        updateStatus(true, "Listening…");
        chunks = [];
        mediaRecorder.start();
        isRecording = true;
      }

      function stopRecording(event) {
        if (!connected || !mediaRecorder || !isRecording) return;
        event.preventDefault();
        talkBtn.classList.remove("recording");
        talkBtn.textContent = "Hold to talk";
        isRecording = false;
        if (mediaRecorder.state === "recording") {
          mediaRecorder.stop();
        }
      }

      window.addEventListener("beforeunload", () => {
        if (audioContext) {
          audioContext.close();
        }
        if (mediaStream) {
          mediaStream.getTracks().forEach((track) => track.stop());
        }
      });
    </script>
  </body>
