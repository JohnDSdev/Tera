<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Gemini Voice AI</title>
  <style>
    :root {
      color-scheme: only light;
      --accent: #2962ff;
      --accent-dark: #0039cb;
      --bg: #ffffff;
      --text: #141414;
      --muted: #667085;
      font-family: "Inter", system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      min-height: 100vh;
      background: var(--bg);
      color: var(--text);
      display: flex;
      justify-content: center;
      align-items: stretch;
      padding: clamp(16px, 5vw, 48px);
    }

    .app {
      width: min(960px, 100%);
      background: #f6f7fb;
      border-radius: 28px;
      box-shadow: 0 32px 70px rgba(20, 20, 43, 0.18);
      overflow: hidden;
      display: grid;
      grid-template-rows: auto 1fr auto;
    }

    header {
      padding: 28px clamp(24px, 5vw, 48px);
      background: linear-gradient(120deg, rgba(41, 98, 255, 0.16), rgba(6, 64, 255, 0));
      border-bottom: 1px solid rgba(15, 23, 42, 0.06);
    }

    header h1 {
      margin: 0 0 6px;
      font-size: clamp(1.6rem, 2.4vw + 1rem, 2.4rem);
      font-weight: 700;
      letter-spacing: -0.04em;
    }

    header p {
      margin: 0;
      color: var(--muted);
      font-size: 0.95rem;
    }

    .connection {
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      margin-top: 18px;
      align-items: center;
    }

    .connection input {
      flex: 1 1 220px;
      padding: 12px 16px;
      border-radius: 14px;
      border: 1px solid rgba(15, 23, 42, 0.12);
      background: #fff;
      font-size: 0.95rem;
    }

    .connection button {
      border: none;
      border-radius: 14px;
      padding: 12px 20px;
      font-weight: 600;
      font-size: 0.95rem;
      cursor: pointer;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
    }

    #connectBtn {
      background: var(--accent);
      color: #fff;
      box-shadow: 0 12px 22px rgba(41, 98, 255, 0.28);
    }

    #connectBtn:disabled {
      background: rgba(41, 98, 255, 0.42);
      cursor: not-allowed;
      box-shadow: none;
    }

    #connectBtn:not(:disabled):hover {
      transform: translateY(-1px);
      box-shadow: 0 18px 28px rgba(41, 98, 255, 0.34);
    }

    .status-bar {
      display: flex;
      align-items: center;
      gap: 12px;
      color: var(--muted);
      font-size: 0.9rem;
    }

    .status-indicator {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: #d1d5db;
      box-shadow: 0 0 0 4px rgba(209, 213, 219, 0.35);
      transition: background 0.3s ease, box-shadow 0.3s ease;
    }

    .status-indicator.online {
      background: #10b981;
      box-shadow: 0 0 0 4px rgba(16, 185, 129, 0.25);
    }

    main {
      display: grid;
      grid-template-columns: minmax(260px, 340px) 1fr;
      gap: clamp(24px, 4vw, 48px);
      padding: clamp(24px, 5vw, 48px);
    }

    @media (max-width: 880px) {
      main {
        grid-template-columns: 1fr;
      }
      .avatar-stage {
        order: -1;
        margin-inline: auto;
      }
    }

    .avatar-stage {
      background: #fff;
      border-radius: 24px;
      padding: 24px;
      display: flex;
      justify-content: center;
      align-items: center;
      position: relative;
      min-height: 320px;
      overflow: hidden;
    }

    .avatar-stage::after {
      content: "";
      position: absolute;
      inset: 24px;
      border-radius: 20px;
      background: radial-gradient(circle at top, rgba(41, 98, 255, 0.12), transparent 60%);
      z-index: 0;
    }

    .avatar-stage img {
      width: min(260px, 100%);
      object-fit: contain;
      position: relative;
      z-index: 1;
      filter: drop-shadow(0 24px 50px rgba(15, 23, 42, 0.25));
      transition: transform 0.4s ease;
    }

    .avatar-stage img.speaking {
      transform: translateY(-6px) scale(1.02);
    }

    .conversation-panel {
      background: #fff;
      border-radius: 24px;
      padding: clamp(20px, 4vw, 32px);
      display: flex;
      flex-direction: column;
      min-height: 320px;
      box-shadow: inset 0 0 0 1px rgba(15, 23, 42, 0.04);
    }

    .messages {
      flex: 1;
      overflow-y: auto;
      padding-right: 8px;
      scrollbar-width: thin;
      display: flex;
      flex-direction: column;
      gap: 18px;
    }

    .messages::-webkit-scrollbar {
      width: 6px;
    }

    .messages::-webkit-scrollbar-thumb {
      background: rgba(41, 98, 255, 0.3);
      border-radius: 999px;
    }

    .bubble {
      padding: 14px 16px;
      border-radius: 18px;
      line-height: 1.48;
      max-width: 100%;
      word-break: break-word;
      font-size: 0.98rem;
      animation: fadeUp 0.35s ease;
    }

    .bubble.user {
      align-self: flex-end;
      background: rgba(41, 98, 255, 0.14);
      color: #1a2a6c;
      border-bottom-right-radius: 4px;
    }

    .bubble.ai {
      align-self: flex-start;
      background: rgba(15, 23, 42, 0.05);
      border-bottom-left-radius: 4px;
    }

    @keyframes fadeUp {
      from {
        opacity: 0;
        transform: translateY(6px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .transcript-note {
      color: var(--muted);
      font-size: 0.85rem;
      margin-top: 4px;
    }

    footer {
      padding: clamp(16px, 4vw, 28px) clamp(24px, 5vw, 48px) 32px;
      border-top: 1px solid rgba(15, 23, 42, 0.06);
      background: #f8f9ff;
      display: flex;
      flex-direction: column;
      gap: 18px;
    }

    .talk-controls {
      display: flex;
      flex-wrap: wrap;
      gap: 16px;
      align-items: center;
      justify-content: space-between;
    }

    #talkBtn {
      border: none;
      border-radius: 999px;
      padding: 18px 42px;
      font-size: 1rem;
      font-weight: 600;
      background: var(--accent);
      color: #fff;
      box-shadow: 0 18px 36px rgba(41, 98, 255, 0.25);
      cursor: pointer;
      transition: transform 0.2s ease, box-shadow 0.2s ease, background 0.2s ease;
    }

    #talkBtn:disabled {
      background: rgba(41, 98, 255, 0.38);
      cursor: not-allowed;
      box-shadow: none;
    }

    #talkBtn.recording {
      background: #ef4444;
      box-shadow: 0 18px 32px rgba(239, 68, 68, 0.35);
    }

    #talkBtn:not(:disabled):active {
      transform: scale(0.97);
    }

    .hint {
      color: var(--muted);
      font-size: 0.85rem;
    }

    audio {
      display: none;
    }
  </style>
</head>
<body>
  <div class="app">
    <header>
      <h1>Gemini Voice AI</h1>
      <p>Hold to talk, release to send. Gemini handles transcription, conversation, and speech back to you.</p>
      <div class="connection">
        <input type="password" id="apiKey" placeholder="Enter your Gemini API key" autocomplete="off">
        <button id="connectBtn">Connect</button>
        <div class="status-bar">
          <span class="status-indicator" id="statusIndicator"></span>
          <span id="statusText">Disconnected</span>
        </div>
      </div>
    </header>

    <main>
      <section class="avatar-stage">
        <img id="avatar" alt="AI avatar" src="a7.png">
      </section>
      <section class="conversation-panel">
        <div class="messages" id="messages"></div>
      </section>
    </main>

    <footer>
      <div class="talk-controls">
        <button id="talkBtn" disabled>Hold to talk</button>
        <span class="hint" id="hintText">Connect with your API key to begin.</span>
      </div>
    </footer>
  </div>

  <audio id="responseAudio"></audio>

  <script>
    const apiKeyInput = document.getElementById('apiKey');
    const connectBtn = document.getElementById('connectBtn');
    const talkBtn = document.getElementById('talkBtn');
    const statusIndicator = document.getElementById('statusIndicator');
    const statusText = document.getElementById('statusText');
    const messagesEl = document.getElementById('messages');
    const hintText = document.getElementById('hintText');
    const avatarEl = document.getElementById('avatar');
    const responseAudio = document.getElementById('responseAudio');

    const startupFrames = ['a1.png', 'a2.png', 'a3.png', 'a4.png', 'a5.png', 'a6.png', 'a7.png'];
    const idleFrame = 'arm_behind_mouth_closed.png';
    const speakingFrames = [
      'arm_out_mouth_open.png',
      'arm_out_mouth_closed.png',
      'arm_behind_mouth_open.png',
      'arm_behind_mouth_closed.png'
    ];

    const GEMINI_ENDPOINT = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent';

    const imageCache = new Map();
    [...startupFrames, idleFrame, ...speakingFrames].forEach(src => {
      const img = new Image();
      img.src = src;
      imageCache.set(src, img);
    });

    let apiKey = '';
    let connected = false;
    let mediaStream = null;
    let mediaRecorder = null;
    let chunks = [];
    let isRecording = false;
    let speakingInterval = null;
    let audioContext = null;
    let recordedMimeType = 'audio/webm';

    const systemInstruction = {
      role: 'system',
      parts: [{ text: 'You are a futuristic but friendly AI guide. Keep responses under 120 words and speak naturally.' }]
    };

    const conversationHistory = [];

    function updateStatus(online, text) {
      statusIndicator.classList.toggle('online', Boolean(online));
      statusText.textContent = text;
    }

    function addMessage(role, text, note) {
      const bubble = document.createElement('div');
      bubble.className = `bubble ${role}`;
      bubble.textContent = text;
      messagesEl.appendChild(bubble);
      if (note) {
        const small = document.createElement('div');
        small.className = 'transcript-note';
        small.textContent = note;
        bubble.appendChild(small);
      }
      messagesEl.scrollTop = messagesEl.scrollHeight;
    }

    function setAvatar(frame, speaking = false) {
      avatarEl.src = frame;
      avatarEl.classList.toggle('speaking', speaking);
    }

    function startAvatarSpeechAnimation() {
      if (speakingInterval) {
        clearInterval(speakingInterval);
      }
      let frameIndex = 0;
      setAvatar(speakingFrames[frameIndex], true);
      speakingInterval = setInterval(() => {
        frameIndex = (frameIndex + 1) % speakingFrames.length;
        const frame = speakingFrames[frameIndex];
        const mouthOpen = frame.includes('open');
        setAvatar(frame, mouthOpen);
      }, 180);
    }

    function stopAvatarSpeechAnimation() {
      if (speakingInterval) {
        clearInterval(speakingInterval);
        speakingInterval = null;
      }
      setAvatar(idleFrame, false);
    }

    async function playStartupSequence() {
      await playStartupSound();
      for (let i = 0; i < startupFrames.length; i++) {
        setAvatar(startupFrames[i]);
        await wait(160);
      }
      setAvatar(idleFrame);
    }

    async function playStartupSound() {
      try {
        if (!audioContext) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        const osc = audioContext.createOscillator();
        const osc2 = audioContext.createOscillator();
        const gain = audioContext.createGain();
        osc.type = 'sawtooth';
        osc.frequency.setValueAtTime(320, audioContext.currentTime);
        osc.frequency.exponentialRampToValueAtTime(880, audioContext.currentTime + 1.1);
        osc2.type = 'triangle';
        osc2.frequency.setValueAtTime(120, audioContext.currentTime);
        osc2.frequency.linearRampToValueAtTime(420, audioContext.currentTime + 1.1);
        gain.gain.setValueAtTime(0.0001, audioContext.currentTime);
        gain.gain.exponentialRampToValueAtTime(0.6, audioContext.currentTime + 0.2);
        gain.gain.exponentialRampToValueAtTime(0.0001, audioContext.currentTime + 1.2);
        osc.connect(gain);
        osc2.connect(gain);
        gain.connect(audioContext.destination);
        osc.start();
        osc2.start();
        osc.stop(audioContext.currentTime + 1.25);
        osc2.stop(audioContext.currentTime + 1.25);
      } catch (err) {
        console.warn('Startup sound failed', err);
      }
    }

    function wait(ms) {
      return new Promise(resolve => setTimeout(resolve, ms));
    }

    connectBtn.addEventListener('click', async () => {
      if (connected) {
        disconnect();
        return;
      }

      const key = apiKeyInput.value.trim();
      if (!key) {
        alert('Please enter your Gemini API key.');
        return;
      }
      if (typeof MediaRecorder === 'undefined') {
        alert('Media recording is not supported in this browser. Try using the latest version of Chrome, Edge, or Firefox.');
        connectBtn.disabled = false;
        hintText.textContent = 'MediaRecorder is unavailable in this browser.';
        return;
      }
      apiKey = key;
      connectBtn.disabled = true;
      hintText.textContent = 'Requesting microphone accessâ€¦';

      try {
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      } catch (err) {
        console.error(err);
        alert('Microphone permission is required to use voice chat.');
        connectBtn.disabled = false;
        hintText.textContent = 'Microphone permission denied.';
        return;
      }

      await playStartupSequence();
      setupRecorder();
      connected = true;
      connectBtn.textContent = 'Disconnect';
      connectBtn.disabled = false;
      talkBtn.disabled = false;
      hintText.textContent = 'Hold the button, speak, then release to send.';
      updateStatus(true, 'Ready');
      conversationHistory.length = 0;
    });

    function disconnect() {
      connected = false;
      apiKey = '';
      connectBtn.textContent = 'Connect';
      talkBtn.disabled = true;
      connectBtn.disabled = false;
      hintText.textContent = 'Connect with your API key to begin.';
      updateStatus(false, 'Disconnected');
      stopAvatarSpeechAnimation();
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      conversationHistory.length = 0;
    }

    function setupRecorder() {
      if (!mediaStream) return;
      const preferredTypes = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/mp4'
      ];
      let chosenType = null;
      if (typeof MediaRecorder !== 'undefined' && typeof MediaRecorder.isTypeSupported === 'function') {
        chosenType = preferredTypes.find(type => MediaRecorder.isTypeSupported(type)) || null;
      }

      try {
        if (chosenType) {
          mediaRecorder = new MediaRecorder(mediaStream, { mimeType: chosenType });
          recordedMimeType = chosenType;
        } else {
          mediaRecorder = new MediaRecorder(mediaStream);
          recordedMimeType = mediaRecorder.mimeType || recordedMimeType;
        }
      } catch (error) {
        console.error('Failed to initialize recorder', error);
        alert('Your browser was unable to start audio recording. Try another browser or update to the latest version.');
        hintText.textContent = 'Recording failed to start.';
        connectBtn.disabled = false;
        talkBtn.disabled = true;
        return;
      }

      mediaRecorder.ondataavailable = event => {
        if (event.data && event.data.size > 0) {
          chunks.push(event.data);
        }
      };
      mediaRecorder.onstop = async () => {
        if (!chunks.length) return;
        const blob = new Blob(chunks, { type: recordedMimeType || 'audio/webm' });
        chunks = [];
        await handleRecording(blob);
      };
    }

    async function handleRecording(blob) {
      updateStatus(true, 'Processing speechâ€¦');
      addMessage('user', 'â€¦', 'Transcribing audio');
      try {
        const transcript = await transcribeWithGemini(blob);
        if (!transcript) {
          throw new Error('No transcript returned');
        }
        replaceLastUserPlaceholder(transcript);
        conversationHistory.push({ role: 'user', parts: [{ text: transcript }] });
        updateStatus(true, 'Thinkingâ€¦');
        const response = await respondWithGemini();
        if (!response) {
          throw new Error('No response from Gemini');
        }
        const { text, audio } = response;
        if (text) {
          addMessage('ai', text);
        } else if (audio) {
          addMessage('ai', 'ðŸ”Š Voice response sent.');
        }
        if (audio) {
          await playResponseAudio(audio);
        } else if (text) {
          await speakWithSpeechSynthesis(text);
        } else {
          stopAvatarSpeechAnimation();
        }
        updateStatus(true, 'Ready');
      } catch (err) {
        console.error(err);
        updateStatus(false, 'Error');
        stopAvatarSpeechAnimation();
        replaceLastUserPlaceholder('Audio could not be processed.');
        addMessage('ai', 'I ran into a problem handling that message. Please try again.');
      }
    }

    function replaceLastUserPlaceholder(text) {
      const bubbles = messagesEl.querySelectorAll('.bubble.user');
      if (!bubbles.length) return;
      const last = bubbles[bubbles.length - 1];
      last.firstChild.textContent = text;
      const note = last.querySelector('.transcript-note');
      if (note) note.remove();
    }

    function cloneHistory() {
      return conversationHistory.map(item => ({
        role: item.role,
        parts: item.parts.map(part => {
          if (part.text !== undefined) {
            return { text: part.text };
          }
          if (part.inlineData) {
            return { inlineData: { ...part.inlineData } };
          }
          return { ...part };
        })
      }));
    }

    async function transcribeWithGemini(blob) {
      const mimeType = blob.type || recordedMimeType || 'audio/webm';
      const base64 = await blobToBase64(blob);
      const body = {
        contents: [
          {
            role: 'user',
            parts: [
              { text: 'Transcribe the following audio faithfully. Respond with text only.' },
              { inlineData: { mimeType, data: base64 } }
            ]
          }
        ]
      };

      const response = await fetch(`${GEMINI_ENDPOINT}?key=${encodeURIComponent(apiKey)}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(body)
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error('Transcription failed: ' + errorText);
      }

      const data = await response.json();
      const text = data?.candidates?.[0]?.content?.parts?.map(part => part.text || '').join('').trim();
      return text || '';
    }

    async function respondWithGemini() {
      const contents = cloneHistory();
      const requestWithAudio = {
        contents,
        systemInstruction,
        responseModalities: ['TEXT', 'AUDIO'],
        audioConfig: {
          audioEncoding: 'AUDIO_ENCODING_OGG_OPUS',
          speakingRate: 1.0
        }
      };

      try {
        const response = await callGemini(requestWithAudio);
        recordModelResponse(response);
        return response;
      } catch (error) {
        if (!isAudioCapabilityError(error)) {
          throw error;
        }
        console.warn('Audio response not available, falling back to text.', error);
        const fallback = await callGemini({
          contents,
          systemInstruction,
          generationConfig: {
            responseMimeType: 'text/plain'
          }
        });
        recordModelResponse(fallback);
        return { text: fallback.text, audio: null };
      }
    }

    function recordModelResponse({ text, audio }) {
      if (text) {
        conversationHistory.push({ role: 'model', parts: [{ text }] });
      } else if (audio) {
        conversationHistory.push({
          role: 'model',
          parts: [{ inlineData: { data: audio.data, mimeType: audio.mimeType } }]
        });
      }
    }

    async function callGemini(body) {
      const response = await fetch(`${GEMINI_ENDPOINT}?key=${encodeURIComponent(apiKey)}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(body)
      });

      if (!response.ok) {
        const errorText = await response.text();
        const err = new Error('Response failed: ' + errorText);
        err.status = response.status;
        throw err;
      }

      const data = await response.json();
      const parts = data?.candidates?.[0]?.content?.parts || [];
      let audio = null;
      const texts = [];
      parts.forEach(part => {
        if (part.text) {
          texts.push(part.text);
        }
        if (part.inlineData && part.inlineData.data) {
          audio = {
            data: part.inlineData.data,
            mimeType: part.inlineData.mimeType || 'audio/ogg'
          };
        }
      });
      return { text: texts.join('').trim(), audio };
    }

    function isAudioCapabilityError(error) {
      const message = (error && error.message) || '';
      return /INVALID_ARGUMENT/i.test(message) || /allowed mimetype/i.test(message);
    }

    async function playResponseAudio(audioPart) {
      const buffer = base64ToArrayBuffer(audioPart.data);
      const blob = new Blob([buffer], { type: audioPart.mimeType || 'audio/ogg' });
      const url = URL.createObjectURL(blob);
      if (!responseAudio.paused) {
        responseAudio.pause();
        responseAudio.currentTime = 0;
      }
      if ('speechSynthesis' in window) {
        speechSynthesis.cancel();
      }
      responseAudio.src = url;
      startAvatarSpeechAnimation();

      await responseAudio.play().catch(err => {
        console.warn('Autoplay blocked', err);
      });

      responseAudio.onended = () => {
        stopAvatarSpeechAnimation();
        URL.revokeObjectURL(url);
      };
    }

    async function speakWithSpeechSynthesis(text) {
      if (!('speechSynthesis' in window) || !text) {
        return;
      }
      return new Promise(resolve => {
        try {
          speechSynthesis.cancel();
          const utterance = new SpeechSynthesisUtterance(text);
          utterance.onstart = () => {
            startAvatarSpeechAnimation();
          };
          const finish = () => {
            stopAvatarSpeechAnimation();
            resolve();
          };
          utterance.onend = finish;
          utterance.onerror = finish;
          speechSynthesis.speak(utterance);
        } catch (err) {
          console.warn('Speech synthesis unavailable', err);
          stopAvatarSpeechAnimation();
          resolve();
        }
      });
    }

    function base64ToArrayBuffer(base64) {
      const binary = atob(base64);
      const len = binary.length;
      const bytes = new Uint8Array(len);
      for (let i = 0; i < len; i++) {
        bytes[i] = binary.charCodeAt(i);
      }
      return bytes.buffer;
    }

    function blobToBase64(blob) {
      return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onloadend = () => {
          const base64 = reader.result.split(',')[1];
          resolve(base64);
        };
        reader.onerror = reject;
        reader.readAsDataURL(blob);
      });
    }

    ['mousedown', 'touchstart'].forEach(eventName => {
      talkBtn.addEventListener(eventName, startRecording);
    });

    ['mouseup', 'mouseleave', 'touchend', 'touchcancel'].forEach(eventName => {
      talkBtn.addEventListener(eventName, stopRecording);
    });

    async function startRecording(event) {
      if (!connected || !mediaRecorder) return;
      event.preventDefault();
      if (mediaRecorder.state === 'recording') return;
      talkBtn.classList.add('recording');
      talkBtn.textContent = 'Listeningâ€¦';
      updateStatus(true, 'Listeningâ€¦');
      addMessage('user', 'â€¦', 'Recording audio');
      chunks = [];
      mediaRecorder.start();
      isRecording = true;
    }

    function stopRecording(event) {
      if (!connected || !mediaRecorder || !isRecording) return;
      event.preventDefault();
      talkBtn.classList.remove('recording');
      talkBtn.textContent = 'Hold to talk';
      isRecording = false;
      if (mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
      }
    }

    window.addEventListener('beforeunload', () => {
      if (audioContext) {
        audioContext.close();
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
      }
    });
  </script>
</body>
</html>
